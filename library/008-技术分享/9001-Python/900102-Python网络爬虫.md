# Python 网络爬虫
>创建人员：**Tony Wang**   
>创建时间：2016-12-08


## 概述

准备以下学习基础：
1. 基本的爬虫工作原理
1. 基本的http抓取工具，scrapy
1. Bloom Filter: Bloom Filters by Example
1. 如果需要大规模网页抓取，你需要学习分布式爬虫的概念。其实没那么玄乎，你只要学会怎样维护一个所有集群机器能够有效分享的分布式队列就好。最简单的实现是python-rq: https://github.com/nvie/rq
1. rq和Scrapy的结合：darkrho/scrapy-redis · GitHub
1. 后续处理，网页析取(grangier/python-goose · GitHub)，存储(Mongodb)


来一段伪代码
```Python
import Queue

initial_page = "http://www.soolife.cn"
url_queue = Queue.Queue()
seen = set()

seen.insert(initial_page)
url_queue.put(initial_page) while(True): #一直进行直到海枯石烂
  if url_queue.size()>0:
    current_url = url_queue.get() #拿出队例中第一个的url
    store(current_url) #把这个url代表的网页存储好
    for next_url in extract_urls(current_url): #提取把这个url里链向的url
      if next_url not in seen:
        seen.put(next_url)
        url_queue.put(next_url)
  else:
    break
```


## 集群化抓取
假设你现在有100台机器可以用，怎么用python实现一个分布式的爬取算法呢？

我们把这100台中的99台运算能力较小的机器叫作slave，另外一台较大的机器叫作master，那么回顾上面代码中的url_queue，如果我们能把这个queue放到这台master机器上，所有的slave都可以通过网络跟master联通，每当一个slave完成下载一个网页，就向master请求一个新的网页来抓取。而每次slave新抓到一个网页，就把这个网页上所有的链接送到master的queue里去。同样，bloom filter也放到master上，但是现在master只发送确定没有被访问过的url给slave。Bloom Filter放到master的内存里，而被访问过的url放到运行在master上的Redis里，这样保证所有操作都是O(1)。（至少平摊是O(1)，Redis的访问效率见:LINSERT – Redis)


### 考虑如何用python实现：
在各台slave上装好scrapy，那么各台机子就变成了一台有抓取能力的slave，在master上装好Redis和rq用作分布式队列。


代码于是写成
```python
#slave.py

current_url = request_from_master()
to_send = []
for next_url in extract_urls(current_url):
    to_send.append(next_url)

store(current_url);
send_to_master(to_send)

#master.py
distributed_queue = DistributedQueue()
bf = BloomFilter()

initial_pages = "www.renmingribao.com"

while(True):
    if request == 'GET':
        if distributed_queue.size()>0:
            send(distributed_queue.get())
        else:
            break
    elif request == 'POST':
        bf.put(request.url)

```
好的，其实你能想到，有人已经给你写好了你需要的：darkrho/scrapy-redis · GitHub


## 使用第三方模块快速抓取与解析
1. Requests 库来代替 urllib,
1. BeautifulSoup 库来代替 re 模块

### Requests Module
Requests 是 Python 界大名鼎鼎的一个网络库, 其设计哲学是为人类而设计, 所以他提供的功能都非常的人性化. 他的方便对我而言主要有两大点:    

>   1. 对 GET 和 POST 方法的封装做的很好, 自动处理了编码等问题;    
>   1. 默认开启了 Cookies 处理, 在处理需要登录的问题上面非常方便.    

Requests 的方便之处不止这两点, 还提供了诸如标准登录接口之类的功能, 我们暂时用不上.    
总而言之, 对于使用过 urllib 的我们来说, 用 requests 会感觉我们之前生活在石器时代. 第三方库的强大就在于这里, 这也是 Python 这么火的重要原因.    


### BeautifulSoup Module

BeautifulSoup 大大方便了我们对抓取的 HTML 数据的解析, 可以用tag, class, id来定位我们想要的东西, 可以直接提取出正文信息, 可以全文搜索, 同样也支持正则表达式, 相当给力.
